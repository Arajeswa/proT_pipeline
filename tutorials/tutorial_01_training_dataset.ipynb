{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Producing a Training Dataset\n",
    "\n",
    "This tutorial guides you through the complete process of generating a training dataset from raw manufacturing data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dataset generation involves two main pipelines:\n",
    "1. **Target Pipeline (IST)**: Processes resistance test data to create `df_trg.csv`\n",
    "2. **Input Pipeline (Process)**: Processes manufacturing parameters and combines with target data\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- Installed all dependencies (`pip install -r requirements.txt`)\n",
    "- Obtained access to the raw data files\n",
    "- Placed data files according to `data/README_DATA.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "First, let's set up the environment and verify the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import exists, join, dirname, abspath\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = dirname(abspath(os.getcwd()))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data structure\n",
    "from proT_pipeline.labels import get_target_dirs, get_input_dirs, get_root_dir\n",
    "\n",
    "ROOT = get_root_dir()\n",
    "TARGET_INPUT, TARGET_BUILDS = get_target_dirs(ROOT)\n",
    "\n",
    "print(\"Checking data directories...\")\n",
    "print(f\"  Target input dir: {TARGET_INPUT}\")\n",
    "print(f\"    Exists: {exists(TARGET_INPUT)}\")\n",
    "print(f\"  Target builds dir: {TARGET_BUILDS}\")\n",
    "print(f\"    Exists: {exists(TARGET_BUILDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Process Target Data (IST)\n",
    "\n",
    "The target pipeline processes the raw IST (Insulation Stress Test) resistance data.\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "| Parameter | Description | Typical Value |\n",
    "|-----------|-------------|---------------|\n",
    "| `build_id` | Output folder name | e.g., \"my_build_2026\" |\n",
    "| `grouping_method` | How to group samples | \"panel\" or \"column\" |\n",
    "| `max_len` | Maximum sequence length | 200 |\n",
    "| `filter_type` | Coupon type filter | \"C\" (canary) or \"P\" (product) |\n",
    "| `max_len_mode` | How to handle long sequences | \"clip\" or \"remove\" |\n",
    "| `mean_bool` | Calculate mean across groups | False |\n",
    "| `std_bool` | Calculate std across groups | False |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for target pipeline\n",
    "BUILD_ID = \"tutorial_build\"          # Change this to your build name\n",
    "GROUPING_METHOD = \"panel\"             # Group by panel (individual samples)\n",
    "MAX_LEN = 200                         # Maximum 200 thermal cycles\n",
    "FILTER_TYPE = \"C\"                     # Canary coupons only\n",
    "UNI_METHOD = \"clip\"                   # Clip for uniform length\n",
    "MAX_LEN_MODE = \"clip\"                 # Clip sequences exceeding max_len\n",
    "MEAN_BOOL = False                     # Don't calculate mean (keep individual measurements)\n",
    "STD_BOOL = False                      # Don't calculate std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run target pipeline\n",
    "from proT_pipeline.target_processing.main import main as target_main\n",
    "\n",
    "print(\"Running target (IST) pipeline...\")\n",
    "print(f\"  Build ID: {BUILD_ID}\")\n",
    "print(f\"  Grouping: {GROUPING_METHOD}\")\n",
    "print(f\"  Max length: {MAX_LEN}\")\n",
    "print()\n",
    "\n",
    "target_main(\n",
    "    build_id=BUILD_ID,\n",
    "    grouping_method=GROUPING_METHOD,\n",
    "    grouping_column=None,\n",
    "    max_len=MAX_LEN,\n",
    "    filter_type=FILTER_TYPE,\n",
    "    uni_method=UNI_METHOD,\n",
    "    max_len_mode=MAX_LEN_MODE,\n",
    "    mean_bool=MEAN_BOOL,\n",
    "    std_bool=STD_BOOL\n",
    ")\n",
    "\n",
    "print(\"\\nTarget pipeline complete!\")\n",
    "print(f\"Output: data/target/builds/{BUILD_ID}/df_trg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify output and preview\n",
    "import pandas as pd\n",
    "\n",
    "df_trg_path = join(TARGET_BUILDS, BUILD_ID, \"df_trg.csv\")\n",
    "print(f\"Loading: {df_trg_path}\")\n",
    "\n",
    "df_trg = pd.read_csv(df_trg_path)\n",
    "print(f\"\\nTarget dataframe shape: {df_trg.shape}\")\n",
    "print(f\"Unique groups (samples): {df_trg['group'].nunique()}\")\n",
    "print(f\"Unique variables: {df_trg['variable'].unique()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_trg.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Copy Target to Build Folder\n",
    "\n",
    "The input pipeline expects `df_trg.csv` in the build's control folder. We need to copy it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for input pipeline\n",
    "DATASET_ID = \"tutorial_build\"         # Must match or create this folder in data/builds/\n",
    "\n",
    "# Create build folder structure if it doesn't exist\n",
    "BUILD_DIR = join(ROOT, \"data\", \"builds\", DATASET_ID)\n",
    "CONTROL_DIR = join(BUILD_DIR, \"control\")\n",
    "OUTPUT_DIR = join(BUILD_DIR, \"output\")\n",
    "\n",
    "os.makedirs(CONTROL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Build directory: {BUILD_DIR}\")\n",
    "print(f\"Control directory: {CONTROL_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df_trg.csv to control folder\n",
    "import shutil\n",
    "\n",
    "source_path = join(TARGET_BUILDS, BUILD_ID, \"df_trg.csv\")\n",
    "dest_path = join(CONTROL_DIR, \"df_trg.csv\")\n",
    "\n",
    "if exists(source_path):\n",
    "    shutil.copy(source_path, dest_path)\n",
    "    print(f\"Copied df_trg.csv to {dest_path}\")\n",
    "else:\n",
    "    print(f\"ERROR: Source file not found: {source_path}\")\n",
    "    print(\"Make sure the target pipeline ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Control Files\n",
    "\n",
    "The input pipeline requires several control files. You should copy these from an existing build or create them according to the schema in `data/builds/template_build/control/README.md`.\n",
    "\n",
    "**Required files:**\n",
    "- `config.yaml` - Points to input dataset folder\n",
    "- `lookup_selected.xlsx` - Variable selection per process\n",
    "- `steps_selected.xlsx` - Process steps to include\n",
    "- `Prozessfolgen_MSEI.xlsx` - Layer/occurrence mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for required control files\n",
    "required_files = [\n",
    "    \"config.yaml\",\n",
    "    \"df_trg.csv\",\n",
    "    \"lookup_selected.xlsx\",\n",
    "    \"steps_selected.xlsx\",\n",
    "    \"Prozessfolgen_MSEI.xlsx\"\n",
    "]\n",
    "\n",
    "print(\"Checking control files:\")\n",
    "all_present = True\n",
    "for f in required_files:\n",
    "    path = join(CONTROL_DIR, f)\n",
    "    status = \"✓\" if exists(path) else \"✗ MISSING\"\n",
    "    if not exists(path):\n",
    "        all_present = False\n",
    "    print(f\"  {status} {f}\")\n",
    "\n",
    "if not all_present:\n",
    "    print(\"\\n⚠️ Some control files are missing!\")\n",
    "    print(\"Copy them from an existing build or create them following the schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Input Pipeline\n",
    "\n",
    "Now we can run the input pipeline to process manufacturing data and generate the final dataset.\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "| Parameter | Description | Typical Value |\n",
    "|-----------|-------------|---------------|\n",
    "| `dataset_id` | Build folder name | Same as your folder in data/builds/ |\n",
    "| `missing_threshold` | Max % missing values per variable | 30 |\n",
    "| `use_stratified_split` | Enable stratified train/test split | True |\n",
    "| `train_ratio` | Proportion for training set | 0.8 |\n",
    "| `n_bins` | Number of bins for stratification | 50 |\n",
    "| `grouping_method` | How samples are grouped | \"panel\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pipeline configuration\n",
    "MISSING_THRESHOLD = 30                # Remove variables with >30% missing values\n",
    "USE_STRATIFIED_SPLIT = True           # Enable stratified splitting\n",
    "STRATIFIED_METRIC = 'rarity_last_value'  # Metric for stratification\n",
    "TRAIN_RATIO = 0.8                     # 80% train, 20% test\n",
    "N_BINS = 50                           # Bins for stratification\n",
    "SPLIT_SHUFFLE = False                 # Don't shuffle within bins\n",
    "SPLIT_SEED = 42                       # Random seed for reproducibility\n",
    "DEBUG = False                         # Set True for quick test with subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run input pipeline\n",
    "from proT_pipeline.main import main as input_main\n",
    "\n",
    "print(\"Running input (process) pipeline...\")\n",
    "print(f\"  Dataset ID: {DATASET_ID}\")\n",
    "print(f\"  Missing threshold: {MISSING_THRESHOLD}%\")\n",
    "print(f\"  Stratified split: {USE_STRATIFIED_SPLIT}\")\n",
    "print(f\"  Train ratio: {TRAIN_RATIO}\")\n",
    "print()\n",
    "\n",
    "input_main(\n",
    "    dataset_id=DATASET_ID,\n",
    "    missing_threshold=MISSING_THRESHOLD,\n",
    "    select_test=False,\n",
    "    use_stratified_split=USE_STRATIFIED_SPLIT,\n",
    "    stratified_metric=STRATIFIED_METRIC,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    n_bins=N_BINS,\n",
    "    split_shuffle=SPLIT_SHUFFLE,\n",
    "    split_seed=SPLIT_SEED,\n",
    "    grouping_method=GROUPING_METHOD,\n",
    "    grouping_column=None,\n",
    "    debug=DEBUG\n",
    ")\n",
    "\n",
    "print(\"\\nInput pipeline complete!\")\n",
    "print(f\"Output: data/builds/{DATASET_ID}/output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Output\n",
    "\n",
    "Let's examine the generated dataset to ensure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the generated dataset\n",
    "dataset_path = join(OUTPUT_DIR, f\"ds_{DATASET_ID}\", \"data.npz\")\n",
    "\n",
    "if exists(dataset_path):\n",
    "    data = np.load(dataset_path)\n",
    "    X = data['x']\n",
    "    Y = data['y']\n",
    "    \n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"\\nX (input) shape: {X.shape}\")\n",
    "    print(f\"  - Samples: {X.shape[0]}\")\n",
    "    print(f\"  - Max sequence length: {X.shape[1]}\")\n",
    "    print(f\"  - Features: {X.shape[2]}\")\n",
    "    print(f\"\\nY (target) shape: {Y.shape}\")\n",
    "    print(f\"  - Samples: {Y.shape[0]}\")\n",
    "    print(f\"  - Max sequence length: {Y.shape[1]}\")\n",
    "    print(f\"  - Features: {Y.shape[2]}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vocabulary files\n",
    "import json\n",
    "\n",
    "vocab_files = [\n",
    "    \"group_vocabulary.json\",\n",
    "    \"process_vocabulary.json\",\n",
    "    \"variables_vocabulary.json_input\",\n",
    "    \"variables_vocabulary.json_trg\",\n",
    "    \"features_dict\"\n",
    "]\n",
    "\n",
    "print(\"Vocabulary files:\")\n",
    "for vf in vocab_files:\n",
    "    path = join(OUTPUT_DIR, vf)\n",
    "    if exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        print(f\"\\n{vf}: {len(vocab)} entries\")\n",
    "        if len(vocab) <= 10:\n",
    "            print(f\"  {vocab}\")\n",
    "        else:\n",
    "            print(f\"  First 5: {dict(list(vocab.items())[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check train/test splits\n",
    "train_path = join(OUTPUT_DIR, f\"ds_{DATASET_ID}\", \"train_data.npz\")\n",
    "test_path = join(OUTPUT_DIR, f\"ds_{DATASET_ID}\", \"test_data.npz\")\n",
    "\n",
    "if exists(train_path) and exists(test_path):\n",
    "    train_data = np.load(train_path)\n",
    "    test_data = np.load(test_path)\n",
    "    \n",
    "    print(\"Train/Test splits:\")\n",
    "    print(f\"  Train samples: {train_data['x'].shape[0]}\")\n",
    "    print(f\"  Test samples: {test_data['x'].shape[0]}\")\n",
    "    print(f\"  Train ratio: {train_data['x'].shape[0] / (train_data['x'].shape[0] + test_data['x'].shape[0]):.2%}\")\n",
    "else:\n",
    "    print(\"Train/test splits not found.\")\n",
    "    print(\"Run with use_stratified_split=True to generate splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully generated a training dataset! The output includes:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `data.npz` | Full dataset (X and Y arrays) |\n",
    "| `train_data.npz` | Training split |\n",
    "| `test_data.npz` | Test split |\n",
    "| `*_vocabulary.json` | Mapping dictionaries |\n",
    "| `features_dict` | Feature index documentation |\n",
    "| `sample_metrics.parquet` | Sample-level metrics for analysis |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Use the dataset for training your transformer model\n",
    "2. See Tutorial 2 for generating prediction datasets (without targets)\n",
    "3. Refer to `INTEGRATION_GUIDE.md` for advanced configuration options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
