{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Producing a Training Dataset\n",
    "\n",
    "This tutorial guides you through the complete process of generating a training dataset from raw manufacturing data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dataset generation involves two main pipelines:\n",
    "1. **Target Pipeline (IST)**: Processes resistance test data to create `df_trg.csv`\n",
    "2. **Input Pipeline (Process)**: Processes manufacturing parameters and combines with target data\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- Installed all dependencies (`pip install -r requirements.txt`)\n",
    "- Obtained access to the raw data files\n",
    "- Placed data files according to `data/README_DATA.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "First, let's set up the environment and verify the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import exists, join, dirname, abspath\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = dirname(abspath(os.getcwd()))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data directories...\n",
      "  Target input dir: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\target\\input\n",
      "    Exists: True\n",
      "  Target builds dir: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\target\\builds\n",
      "    Exists: True\n"
     ]
    }
   ],
   "source": [
    "# Verify data structure\n",
    "from proT_pipeline.labels import get_target_dirs, get_input_dirs, get_root_dir\n",
    "\n",
    "ROOT = get_root_dir()\n",
    "TARGET_INPUT, TARGET_BUILDS = get_target_dirs(ROOT)\n",
    "\n",
    "print(\"Checking data directories...\")\n",
    "print(f\"  Target input dir: {TARGET_INPUT}\")\n",
    "print(f\"    Exists: {exists(TARGET_INPUT)}\")\n",
    "print(f\"  Target builds dir: {TARGET_BUILDS}\")\n",
    "print(f\"    Exists: {exists(TARGET_BUILDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Process Target Data (IST)\n",
    "\n",
    "The target pipeline processes the raw IST (Insulation Stress Test) resistance data.\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "| Parameter | Description | Typical Value |\n",
    "|-----------|-------------|---------------|\n",
    "| `build_id` | Output folder name | e.g., \"my_build_2026\" |\n",
    "| `grouping_method` | How to group samples | \"panel\" or \"column\" |\n",
    "| `max_len` | Maximum sequence length | 200 |\n",
    "| `filter_type` | Coupon type filter | \"C\" (canary) or \"P\" (product) |\n",
    "| `max_len_mode` | How to handle long sequences | \"clip\" or \"remove\" |\n",
    "| `mean_bool` | Calculate mean across groups | False |\n",
    "| `std_bool` | Calculate std across groups | False |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for target pipeline\n",
    "BUILD_ID = \"tutorial_build\"          # Change this to your build name\n",
    "GROUPING_METHOD = \"panel\"             # Group by panel (individual samples)\n",
    "MAX_LEN = 200                         # Maximum 200 thermal cycles\n",
    "FILTER_TYPE = \"C\"                     # Canary coupons only\n",
    "UNI_METHOD = \"clip\"                   # Clip for uniform length\n",
    "MAX_LEN_MODE = \"clip\"                 # Clip sequences exceeding max_len\n",
    "MEAN_BOOL = False                     # Don't calculate mean (keep individual measurements)\n",
    "STD_BOOL = False                      # Don't calculate std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running target (IST) pipeline...\n",
      "  Build ID: tutorial_build\n",
      "  Grouping: panel\n",
      "  Max length: 200\n",
      "\n",
      "Processing ist dataframe...\n",
      "Normalizing values...\n",
      "Some groups have multiple ids\n",
      "group\n",
      "CUFR_28    2\n",
      "CVGW_13    2\n",
      "CVGW_15    2\n",
      "CVGW_17    2\n",
      "CVGW_21    2\n",
      "CVGW_30    2\n",
      "CVGW_44    2\n",
      "CVGW_47    2\n",
      "CVGW_8     2\n",
      "Name: id, dtype: int64\n",
      "Since mean_bool=False, proceed selecting the dominating one\n",
      "Each group has exactly one unique id now.\n",
      "Target filtered dataframe assembled: index unique: True\n",
      "\n",
      "Target pipeline complete!\n",
      "Output: data/target/builds/tutorial_build/df_trg.csv\n"
     ]
    }
   ],
   "source": [
    "# Run target pipeline\n",
    "from proT_pipeline.target_processing.main import main as target_main\n",
    "\n",
    "print(\"Running target (IST) pipeline...\")\n",
    "print(f\"  Build ID: {BUILD_ID}\")\n",
    "print(f\"  Grouping: {GROUPING_METHOD}\")\n",
    "print(f\"  Max length: {MAX_LEN}\")\n",
    "print()\n",
    "\n",
    "target_main(\n",
    "    build_id=BUILD_ID,\n",
    "    grouping_method=GROUPING_METHOD,\n",
    "    grouping_column=None,\n",
    "    max_len=MAX_LEN,\n",
    "    filter_type=FILTER_TYPE,\n",
    "    uni_method=UNI_METHOD,\n",
    "    max_len_mode=MAX_LEN_MODE,\n",
    "    mean_bool=MEAN_BOOL,\n",
    "    std_bool=STD_BOOL\n",
    ")\n",
    "\n",
    "print(\"\\nTarget pipeline complete!\")\n",
    "print(f\"Output: data/target/builds/{BUILD_ID}/df_trg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\target\\builds\\tutorial_build\\df_trg.csv\n",
      "\n",
      "Target dataframe shape: (782267, 10)\n",
      "Unique groups (samples): 1957\n",
      "Unique variables: ['delta_A_norm' 'delta_B_norm']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Unnamed: 0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "position",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "design",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "version",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "variable",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "value",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2909cdfa-de25-4af7-a6c6-217e3a3a0511",
       "rows": [
        [
         "0",
         "0",
         "354905",
         "CFWQ_10",
         "1.0",
         "49135",
         "2023-08-12 08:38:00",
         "453828",
         "B",
         "delta_A_norm",
         "0.0"
        ],
        [
         "1",
         "1",
         "354906",
         "CFWQ_10",
         "2.0",
         "49135",
         "2023-08-12 08:38:00",
         "453828",
         "B",
         "delta_A_norm",
         "0.0037042524818486"
        ],
        [
         "2",
         "2",
         "354907",
         "CFWQ_10",
         "3.0",
         "49135",
         "2023-08-12 08:38:00",
         "453828",
         "B",
         "delta_A_norm",
         "0.0125944584382864"
        ],
        [
         "3",
         "3",
         "354908",
         "CFWQ_10",
         "4.0",
         "49135",
         "2023-08-12 08:38:00",
         "453828",
         "B",
         "delta_A_norm",
         "0.0125944584382864"
        ],
        [
         "4",
         "4",
         "354909",
         "CFWQ_10",
         "5.0",
         "49135",
         "2023-08-12 08:38:00",
         "453828",
         "B",
         "delta_A_norm",
         "0.020743813898354"
        ],
        [
         "5",
         "5",
         "354910",
         "CFWQ_10",
         "6.0",
         "49135",
         "2023-08-12 08:38:00",
         "453828",
         "B",
         "delta_A_norm",
         "0.0148170099273969"
        ],
        [
         "6",
         "6",
         "354911",
         "CFWQ_10",
         "7.0",
         "49135",
         "2023-08-12 08:38:00",
         "453828",
         "B",
         "delta_A_norm",
         "0.0244480663802049"
        ],
        [
         "7",
         "7",
         "354912",
         "CFWQ_10",
         "8.0",
         "49135",
         "2023-08-12 08:38:00",
         "453828",
         "B",
         "delta_A_norm",
         "0.0266706178693154"
        ],
        [
         "8",
         "8",
         "354913",
         "CFWQ_10",
         "9.0",
         "49135",
         "2023-08-12 08:38:00",
         "453828",
         "B",
         "delta_A_norm",
         "0.0303748703511641"
        ],
        [
         "9",
         "9",
         "354914",
         "CFWQ_10",
         "10.0",
         "49135",
         "2023-08-12 08:38:00",
         "453828",
         "B",
         "delta_A_norm",
         "0.0318565713439022"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>group</th>\n",
       "      <th>position</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>design</th>\n",
       "      <th>version</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>354905</td>\n",
       "      <td>CFWQ_10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49135</td>\n",
       "      <td>2023-08-12 08:38:00</td>\n",
       "      <td>453828</td>\n",
       "      <td>B</td>\n",
       "      <td>delta_A_norm</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>354906</td>\n",
       "      <td>CFWQ_10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49135</td>\n",
       "      <td>2023-08-12 08:38:00</td>\n",
       "      <td>453828</td>\n",
       "      <td>B</td>\n",
       "      <td>delta_A_norm</td>\n",
       "      <td>0.003704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>354907</td>\n",
       "      <td>CFWQ_10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49135</td>\n",
       "      <td>2023-08-12 08:38:00</td>\n",
       "      <td>453828</td>\n",
       "      <td>B</td>\n",
       "      <td>delta_A_norm</td>\n",
       "      <td>0.012594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>354908</td>\n",
       "      <td>CFWQ_10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>49135</td>\n",
       "      <td>2023-08-12 08:38:00</td>\n",
       "      <td>453828</td>\n",
       "      <td>B</td>\n",
       "      <td>delta_A_norm</td>\n",
       "      <td>0.012594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>354909</td>\n",
       "      <td>CFWQ_10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49135</td>\n",
       "      <td>2023-08-12 08:38:00</td>\n",
       "      <td>453828</td>\n",
       "      <td>B</td>\n",
       "      <td>delta_A_norm</td>\n",
       "      <td>0.020744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>354910</td>\n",
       "      <td>CFWQ_10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>49135</td>\n",
       "      <td>2023-08-12 08:38:00</td>\n",
       "      <td>453828</td>\n",
       "      <td>B</td>\n",
       "      <td>delta_A_norm</td>\n",
       "      <td>0.014817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>354911</td>\n",
       "      <td>CFWQ_10</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49135</td>\n",
       "      <td>2023-08-12 08:38:00</td>\n",
       "      <td>453828</td>\n",
       "      <td>B</td>\n",
       "      <td>delta_A_norm</td>\n",
       "      <td>0.024448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>354912</td>\n",
       "      <td>CFWQ_10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>49135</td>\n",
       "      <td>2023-08-12 08:38:00</td>\n",
       "      <td>453828</td>\n",
       "      <td>B</td>\n",
       "      <td>delta_A_norm</td>\n",
       "      <td>0.026671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>354913</td>\n",
       "      <td>CFWQ_10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>49135</td>\n",
       "      <td>2023-08-12 08:38:00</td>\n",
       "      <td>453828</td>\n",
       "      <td>B</td>\n",
       "      <td>delta_A_norm</td>\n",
       "      <td>0.030375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>354914</td>\n",
       "      <td>CFWQ_10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>49135</td>\n",
       "      <td>2023-08-12 08:38:00</td>\n",
       "      <td>453828</td>\n",
       "      <td>B</td>\n",
       "      <td>delta_A_norm</td>\n",
       "      <td>0.031857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   index    group  position     id                 date  design  \\\n",
       "0           0  354905  CFWQ_10       1.0  49135  2023-08-12 08:38:00  453828   \n",
       "1           1  354906  CFWQ_10       2.0  49135  2023-08-12 08:38:00  453828   \n",
       "2           2  354907  CFWQ_10       3.0  49135  2023-08-12 08:38:00  453828   \n",
       "3           3  354908  CFWQ_10       4.0  49135  2023-08-12 08:38:00  453828   \n",
       "4           4  354909  CFWQ_10       5.0  49135  2023-08-12 08:38:00  453828   \n",
       "5           5  354910  CFWQ_10       6.0  49135  2023-08-12 08:38:00  453828   \n",
       "6           6  354911  CFWQ_10       7.0  49135  2023-08-12 08:38:00  453828   \n",
       "7           7  354912  CFWQ_10       8.0  49135  2023-08-12 08:38:00  453828   \n",
       "8           8  354913  CFWQ_10       9.0  49135  2023-08-12 08:38:00  453828   \n",
       "9           9  354914  CFWQ_10      10.0  49135  2023-08-12 08:38:00  453828   \n",
       "\n",
       "  version      variable     value  \n",
       "0       B  delta_A_norm  0.000000  \n",
       "1       B  delta_A_norm  0.003704  \n",
       "2       B  delta_A_norm  0.012594  \n",
       "3       B  delta_A_norm  0.012594  \n",
       "4       B  delta_A_norm  0.020744  \n",
       "5       B  delta_A_norm  0.014817  \n",
       "6       B  delta_A_norm  0.024448  \n",
       "7       B  delta_A_norm  0.026671  \n",
       "8       B  delta_A_norm  0.030375  \n",
       "9       B  delta_A_norm  0.031857  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify output and preview\n",
    "import pandas as pd\n",
    "\n",
    "df_trg_path = join(TARGET_BUILDS, BUILD_ID, \"df_trg.csv\")\n",
    "print(f\"Loading: {df_trg_path}\")\n",
    "\n",
    "df_trg = pd.read_csv(df_trg_path)\n",
    "print(f\"\\nTarget dataframe shape: {df_trg.shape}\")\n",
    "print(f\"Unique groups (samples): {df_trg['group'].nunique()}\")\n",
    "print(f\"Unique variables: {df_trg['variable'].unique()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_trg.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Copy Target to Build Folder\n",
    "\n",
    "The input pipeline expects `df_trg.csv` in the build's control folder. We need to copy it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build directory: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\builds\\tutorial_build\n",
      "Control directory: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\builds\\tutorial_build\\control\n",
      "Output directory: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\builds\\tutorial_build\\output\n"
     ]
    }
   ],
   "source": [
    "# Configuration for input pipeline\n",
    "DATASET_ID = \"tutorial_build\"         # Must match or create this folder in data/builds/\n",
    "\n",
    "# Create build folder structure if it doesn't exist\n",
    "BUILD_DIR = join(ROOT, \"data\", \"builds\", DATASET_ID)\n",
    "CONTROL_DIR = join(BUILD_DIR, \"control\")\n",
    "OUTPUT_DIR = join(BUILD_DIR, \"output\")\n",
    "\n",
    "os.makedirs(CONTROL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Build directory: {BUILD_DIR}\")\n",
    "print(f\"Control directory: {CONTROL_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied df_trg.csv to c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\builds\\tutorial_build\\control\\df_trg.csv\n"
     ]
    }
   ],
   "source": [
    "# Copy df_trg.csv to control folder\n",
    "import shutil\n",
    "\n",
    "source_path = join(TARGET_BUILDS, BUILD_ID, \"df_trg.csv\")\n",
    "dest_path = join(CONTROL_DIR, \"df_trg.csv\")\n",
    "\n",
    "if exists(source_path):\n",
    "    shutil.copy(source_path, dest_path)\n",
    "    print(f\"Copied df_trg.csv to {dest_path}\")\n",
    "else:\n",
    "    print(f\"ERROR: Source file not found: {source_path}\")\n",
    "    print(\"Make sure the target pipeline ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Control Files\n",
    "\n",
    "The input pipeline requires several control files. You should copy these from an existing build or create them according to the schema in `data/builds/template_build/control/README.md`.\n",
    "\n",
    "**Required files:**\n",
    "- `config.yaml` - Points to input dataset folder\n",
    "- `lookup_selected.xlsx` - Variable selection per process\n",
    "- `steps_selected.xlsx` - Process steps to include\n",
    "- `Prozessfolgen_MSEI.xlsx` - Layer/occurrence mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking control files:\n",
      "  ✓ config.yaml\n",
      "  ✓ df_trg.csv\n",
      "  ✓ lookup_selected.xlsx\n",
      "  ✓ steps_selected.xlsx\n",
      "  ✓ Prozessfolgen_MSEI.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Check for required control files\n",
    "required_files = [\n",
    "    \"config.yaml\",\n",
    "    \"df_trg.csv\",\n",
    "    \"lookup_selected.xlsx\",\n",
    "    \"steps_selected.xlsx\",\n",
    "    \"Prozessfolgen_MSEI.xlsx\"\n",
    "]\n",
    "\n",
    "print(\"Checking control files:\")\n",
    "all_present = True\n",
    "for f in required_files:\n",
    "    path = join(CONTROL_DIR, f)\n",
    "    status = \"✓\" if exists(path) else \"✗ MISSING\"\n",
    "    if not exists(path):\n",
    "        all_present = False\n",
    "    print(f\"  {status} {f}\")\n",
    "\n",
    "if not all_present:\n",
    "    print(\"\\n⚠️ Some control files are missing!\")\n",
    "    print(\"Copy them from an existing build or create them following the schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Input Pipeline\n",
    "\n",
    "Now we can run the input pipeline to process manufacturing data and generate the final dataset.\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "| Parameter | Description | Typical Value |\n",
    "|-----------|-------------|---------------|\n",
    "| `dataset_id` | Build folder name | Same as your folder in data/builds/ |\n",
    "| `missing_threshold` | Max % missing values per variable | 30 |\n",
    "| `use_stratified_split` | Enable stratified train/test split | True |\n",
    "| `train_ratio` | Proportion for training set | 0.8 |\n",
    "| `n_bins` | Number of bins for stratification | 50 |\n",
    "| `grouping_method` | How samples are grouped | \"panel\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pipeline configuration\n",
    "MISSING_THRESHOLD = 30                # Remove variables with >30% missing values\n",
    "USE_STRATIFIED_SPLIT = True           # Enable stratified splitting\n",
    "STRATIFIED_METRIC = 'rarity_last_value'  # Metric for stratification\n",
    "TRAIN_RATIO = 0.8                     # 80% train, 20% test\n",
    "N_BINS = 50                           # Bins for stratification\n",
    "SPLIT_SHUFFLE = False                 # Don't shuffle within bins\n",
    "SPLIT_SEED = 42                       # Random seed for reproducibility\n",
    "DEBUG = False                         # Set True for quick test with subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running input (process) pipeline...\n",
      "  Dataset ID: tutorial_build\n",
      "  Missing threshold: 30%\n",
      "  Stratified split: True\n",
      "  Train ratio: 0.8\n",
      "\n",
      "Galvanic WARNING: UCL_Spüle Bakterienbefall-1.04 not in the columns\n",
      "Error occurred 'UCL_Spüle Bakterienbefall-1.04'\n",
      "Galvanic WARNING: UCL_Galv. Cu Cl--1.11/1.12 not in the columns\n",
      "Error occurred 'UCL_Galv. Cu Cl--1.11/1.12'\n",
      "Galvanic WARNING: UCL_Galv. Cu Cl--1.09/1.10 not in the columns\n",
      "Error occurred 'UCL_Galv. Cu Cl--1.09/1.10'\n",
      "Galvanic WARNING: 25TrocknerUmsetzzeit not in the columns\n",
      "Error occurred '25TrocknerUmsetzzeit'\n",
      "Galvanic WARNING: 25TrocknerDelta_time not in the columns\n",
      "Error occurred '25TrocknerDelta_time'\n",
      "Galvanic WARNING: 25TrocknerDelta_time_% not in the columns\n",
      "Error occurred '25TrocknerDelta_time_%'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\proT_pipeline\\input_processing\\data_loader.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_['numeric_part'] = df_[process.panel_label].astype(str).str.extract(r'(\\d+)')[0].astype(\"Int64\")\n",
      "c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\proT_pipeline\\input_processing\\data_loader.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_[trans_group_id] = df_[process.WA_label] + '_' + df_['numeric_part'].astype(str)\n",
      "c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\proT_pipeline\\input_processing\\data_loader.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_[trans_group_id] = df_[process.WA_label] + '_*'\n",
      "100%|██████████| 1920/1920 [05:11<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening successful, dataset correctly generated!\n",
      "Found the following sequence lengths\n",
      "        length_count                                                ids\n",
      "length                                                                 \n",
      "111                1                                                153\n",
      "127                5                            148, 149, 150, 152, 155\n",
      "305               10   151, 154, 796, 797, 798, 799, 800, 801, 802, 803\n",
      "312                3                                    141, 1355, 1365\n",
      "330                1                                               1798\n",
      "...              ...                                                ...\n",
      "989              163  34, 35, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47...\n",
      "991               10  949, 1473, 1566, 1670, 1740, 1741, 1742, 1790,...\n",
      "1007               8      416, 1841, 1842, 1843, 1844, 1845, 1847, 1848\n",
      "1015              54  8, 9, 12, 14, 15, 105, 106, 108, 186, 188, 189...\n",
      "1023             224  3, 48, 49, 50, 51, 53, 54, 55, 118, 119, 120, ...\n",
      "\n",
      "[131 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1920/1920 [02:13<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening successful, dataset correctly generated!\n",
      "Found the following sequence lengths\n",
      "        length_count                                                ids\n",
      "length                                                                 \n",
      "200                1                                                656\n",
      "236                1                                               1899\n",
      "335                1                                               1473\n",
      "363                1                                               1807\n",
      "381                1                                               1468\n",
      "383                1                                                739\n",
      "384                1                                               1758\n",
      "390                1                                               1598\n",
      "397                1                                                733\n",
      "399                2                                          573, 1134\n",
      "400             1909  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ...\n",
      "Saved train dataset to train_data.npz (X: (1526, 1023, 12), Y: (1526, 400, 9))\n",
      "Saved test dataset to test_data.npz (X: (394, 1023, 12), Y: (394, 400, 9))\n",
      "\n",
      "Input pipeline complete!\n",
      "Output: data/builds/tutorial_build/output/\n"
     ]
    }
   ],
   "source": [
    "# Run input pipeline\n",
    "from proT_pipeline.main import main as input_main\n",
    "\n",
    "print(\"Running input (process) pipeline...\")\n",
    "print(f\"  Dataset ID: {DATASET_ID}\")\n",
    "print(f\"  Missing threshold: {MISSING_THRESHOLD}%\")\n",
    "print(f\"  Stratified split: {USE_STRATIFIED_SPLIT}\")\n",
    "print(f\"  Train ratio: {TRAIN_RATIO}\")\n",
    "print()\n",
    "\n",
    "input_main(\n",
    "    dataset_id=DATASET_ID,\n",
    "    missing_threshold=MISSING_THRESHOLD,\n",
    "    select_test=False,\n",
    "    use_stratified_split=USE_STRATIFIED_SPLIT,\n",
    "    stratified_metric=STRATIFIED_METRIC,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    n_bins=N_BINS,\n",
    "    split_shuffle=SPLIT_SHUFFLE,\n",
    "    split_seed=SPLIT_SEED,\n",
    "    grouping_method=GROUPING_METHOD,\n",
    "    grouping_column=None,\n",
    "    debug=DEBUG\n",
    ")\n",
    "\n",
    "print(\"\\nInput pipeline complete!\")\n",
    "print(f\"Output: data/builds/{DATASET_ID}/output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Output\n",
    "\n",
    "Let's examine the generated dataset to ensure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "\n",
      "X (input) shape: (1920, 1023, 12)\n",
      "  - Samples: 1920\n",
      "  - Max sequence length: 1023\n",
      "  - Features: 12\n",
      "\n",
      "Y (target) shape: (1920, 400, 9)\n",
      "  - Samples: 1920\n",
      "  - Max sequence length: 400\n",
      "  - Features: 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the generated dataset\n",
    "dataset_path = join(OUTPUT_DIR, f\"ds_{DATASET_ID}\", \"data.npz\")\n",
    "\n",
    "if exists(dataset_path):\n",
    "    data = np.load(dataset_path)\n",
    "    X = data['x']\n",
    "    Y = data['y']\n",
    "    \n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"\\nX (input) shape: {X.shape}\")\n",
    "    print(f\"  - Samples: {X.shape[0]}\")\n",
    "    print(f\"  - Max sequence length: {X.shape[1]}\")\n",
    "    print(f\"  - Features: {X.shape[2]}\")\n",
    "    print(f\"\\nY (target) shape: {Y.shape}\")\n",
    "    print(f\"  - Samples: {Y.shape[0]}\")\n",
    "    print(f\"  - Max sequence length: {Y.shape[1]}\")\n",
    "    print(f\"  - Features: {Y.shape[2]}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary files:\n",
      "\n",
      "group_vocabulary.json: 1920 entries\n",
      "  First 5: {'CUEX_13': 0, 'CUEX_17': 1, 'CUEX_20': 2, 'CUEX_28': 3, 'CUEX_34': 4}\n",
      "\n",
      "process_vocabulary.json: 5 entries\n",
      "  {'Laser': 1, 'Plasma': 2, 'Galvanic': 3, 'Multibond': 4, 'Microetch': 5}\n",
      "\n",
      "variables_vocabulary.json_input: 372 entries\n",
      "  First 5: {'las_11': 1, 'las_12': 2, 'las_13': 3, 'las_15': 4, 'las_16': 5}\n",
      "\n",
      "variables_vocabulary.json_trg: 2 entries\n",
      "  {'delta_A_norm': 1, 'delta_B_norm': 2}\n",
      "\n",
      "features_dict: 2 entries\n",
      "  {'input': {'0': 'group', '1': 'process', '2': 'occurrence', '3': 'step', '4': 'variable', '5': 'value_norm', '6': 'order', '7': 'year', '8': 'month', '9': 'day', '10': 'hour', '11': 'minute'}, 'target': {'0': 'group', '1': 'position', '2': 'variable', '3': 'value', '4': 'year', '5': 'month', '6': 'day', '7': 'hour', '8': 'minute'}}\n"
     ]
    }
   ],
   "source": [
    "# Check vocabulary files\n",
    "import json\n",
    "\n",
    "vocab_files = [\n",
    "    \"group_vocabulary.json\",\n",
    "    \"process_vocabulary.json\",\n",
    "    \"variables_vocabulary.json_input\",\n",
    "    \"variables_vocabulary.json_trg\",\n",
    "    \"features_dict\"\n",
    "]\n",
    "\n",
    "print(\"Vocabulary files:\")\n",
    "for vf in vocab_files:\n",
    "    path = join(OUTPUT_DIR, vf)\n",
    "    if exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        print(f\"\\n{vf}: {len(vocab)} entries\")\n",
    "        if len(vocab) <= 10:\n",
    "            print(f\"  {vocab}\")\n",
    "        else:\n",
    "            print(f\"  First 5: {dict(list(vocab.items())[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test splits:\n",
      "  Train samples: 1526\n",
      "  Test samples: 394\n",
      "  Train ratio: 79.48%\n"
     ]
    }
   ],
   "source": [
    "# Check train/test splits\n",
    "train_path = join(OUTPUT_DIR, f\"ds_{DATASET_ID}\", \"train_data.npz\")\n",
    "test_path = join(OUTPUT_DIR, f\"ds_{DATASET_ID}\", \"test_data.npz\")\n",
    "\n",
    "if exists(train_path) and exists(test_path):\n",
    "    train_data = np.load(train_path)\n",
    "    test_data = np.load(test_path)\n",
    "    \n",
    "    print(\"Train/Test splits:\")\n",
    "    print(f\"  Train samples: {train_data['x'].shape[0]}\")\n",
    "    print(f\"  Test samples: {test_data['x'].shape[0]}\")\n",
    "    print(f\"  Train ratio: {train_data['x'].shape[0] / (train_data['x'].shape[0] + test_data['x'].shape[0]):.2%}\")\n",
    "else:\n",
    "    print(\"Train/test splits not found.\")\n",
    "    print(\"Run with use_stratified_split=True to generate splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully generated a training dataset! The output includes:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `data.npz` | Full dataset (X and Y arrays) |\n",
    "| `train_data.npz` | Training split |\n",
    "| `test_data.npz` | Test split |\n",
    "| `*_vocabulary.json` | Mapping dictionaries |\n",
    "| `features_dict` | Feature index documentation |\n",
    "| `sample_metrics.parquet` | Sample-level metrics for analysis |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Use the dataset for training your transformer model\n",
    "2. See Tutorial 2 for generating prediction datasets (without targets)\n",
    "3. Refer to `INTEGRATION_GUIDE.md` for advanced configuration options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prochain_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
