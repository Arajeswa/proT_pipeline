{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Producing a Dataset for Prediction\n",
    "\n",
    "This tutorial guides you through generating a dataset for **inference/prediction** mode, where target (IST) data is not available.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dataset generation follows the same structure as Tutorial 1:\n",
    "1. **Target Pipeline (IST)**: ⚠️ **DIFFERENCE**: Instead of processing real IST data, we generate placeholders\n",
    "2. **Input Pipeline (Process)**: Processes manufacturing parameters (same as Tutorial 1)\n",
    "\n",
    "## Key Difference from Training\n",
    "\n",
    "| Aspect | Tutorial 1 (Training) | Tutorial 2 (Prediction) |\n",
    "|--------|----------------------|-------------------------|\n",
    "| Step 1 | Process real IST data | Generate placeholder IST |\n",
    "| Y values | Actual resistance deltas | Zeros (placeholders) |\n",
    "| Stratified split | Enabled | Disabled |\n",
    "| Purpose | Train model | Generate predictions |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- Installed all dependencies (`pip install -r requirements.txt`)\n",
    "- Obtained access to the raw process data files\n",
    "- Placed data files according to `data/README_DATA.md`\n",
    "- A list of sample groups (batches/panels) to generate predictions for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "First, let's set up the environment and verify the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import exists, join, dirname, abspath\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = dirname(abspath(os.getcwd()))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data directories...\n",
      "  Target input dir: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\target\\input\n",
      "    Exists: True\n",
      "  Target builds dir: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\target\\builds\n",
      "    Exists: True\n"
     ]
    }
   ],
   "source": [
    "# Verify data structure\n",
    "from proT_pipeline.labels import get_target_dirs, get_input_dirs, get_root_dir\n",
    "\n",
    "ROOT = get_root_dir()\n",
    "TARGET_INPUT, TARGET_BUILDS = get_target_dirs(ROOT)\n",
    "\n",
    "print(\"Checking data directories...\")\n",
    "print(f\"  Target input dir: {TARGET_INPUT}\")\n",
    "print(f\"    Exists: {exists(TARGET_INPUT)}\")\n",
    "print(f\"  Target builds dir: {TARGET_BUILDS}\")\n",
    "print(f\"    Exists: {exists(TARGET_BUILDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Process Target Data (IST)\n",
    "\n",
    "⚠️ **PREDICTION MODE DIFFERENCE**: Instead of processing real IST data with `target_main()`, we generate **placeholder targets** using `generate_ist_placeholders()`.\n",
    "\n",
    "The placeholder targets have:\n",
    "- Same structure as real IST data (group, position, variable, value columns)\n",
    "- Zero values instead of actual resistance measurements\n",
    "- Same sequence length (`MAX_LEN`) as training data\n",
    "\n",
    "### Input Options for Group IDs\n",
    "\n",
    "The `generate_ist_placeholders` function accepts group IDs in multiple formats:\n",
    "- **List**: `[\"CYDH_01\", \"CYDH_05\", ...]`\n",
    "- **CSV file**: Path to file with 'group' column\n",
    "- **NPY file**: Path to numpy array\n",
    "- **TXT file**: Path to text file (one ID per line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for target pipeline (PREDICTION MODE)\n",
    "BUILD_ID = \"prediction_tutorial\"      # Change this to your build name\n",
    "GROUPING_METHOD = \"panel\"             # Group by panel (must match training)\n",
    "MAX_LEN = 200                         # Maximum sequence length (must match training)\n",
    "NUM_VARS = 2                          # Number of target variables (Sense A, B)\n",
    "\n",
    "# Reference build for copying control files (Step 3)\n",
    "REFERENCE_BUILD = \"dyconex_251117\"    # An existing build with control files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups to predict: 16\n"
     ]
    }
   ],
   "source": [
    "# Define the group IDs to generate predictions for\n",
    "# Option A: Define as a list\n",
    "PREDICTION_GROUPS = [\n",
    "    \"CYDH_01\",\n",
    "    \"CYDH_05\",\n",
    "    \"CYDH_13\",\n",
    "    \"CYDH_15\",\n",
    "    \"CYDH_22\",\n",
    "    \"CYDH_28\",\n",
    "    \"CYDH_30\",\n",
    "    \"CYDH_38\",\n",
    "    \"CYEI_04\",\n",
    "    \"CYEI_10\",\n",
    "    \"CYEI_20\",\n",
    "    \"CYEI_26\",\n",
    "    \"CYEI_28\",\n",
    "    \"CYEI_29\",\n",
    "    \"CYEI_39\",\n",
    "    \"CYEI_41\",\n",
    "]\n",
    "\n",
    "# Option B: Load from file (uncomment to use)\n",
    "# PREDICTION_GROUPS = \"path/to/sample_ids.csv\"    # CSV with 'group' column\n",
    "# PREDICTION_GROUPS = \"path/to/sample_ids.npy\"    # NumPy array\n",
    "# PREDICTION_GROUPS = \"path/to/sample_ids.txt\"    # Text file, one ID per line\n",
    "\n",
    "# Option C: Load from existing dataset (uncomment to use)\n",
    "# from proT_pipeline.target_processing.placeholders import load_group_ids_from_process_data\n",
    "# PREDICTION_GROUPS = load_group_ids_from_process_data(\"dyconex_251117\", limit=20)\n",
    "\n",
    "print(f\"Groups to predict: {len(PREDICTION_GROUPS) if isinstance(PREDICTION_GROUPS, list) else PREDICTION_GROUPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating placeholder targets (PREDICTION MODE)...\n",
      "  Build ID: prediction_tutorial\n",
      "  Max length: 200\n",
      "  Num variables: 2\n",
      "\n",
      "Saved placeholder target to: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\target\\builds\\prediction_tutorial\\df_trg.csv\n",
      "\n",
      "Placeholder target generation complete!\n",
      "Output: data/target/builds/prediction_tutorial/df_trg.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate placeholder targets (PREDICTION MODE)\n",
    "# This replaces the target_main() call from Tutorial 1\n",
    "from proT_pipeline.target_processing.placeholders import generate_ist_placeholders\n",
    "from os import makedirs\n",
    "\n",
    "# Create target build directory\n",
    "TARGET_BUILD_DIR = join(TARGET_BUILDS, BUILD_ID)\n",
    "makedirs(TARGET_BUILD_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Generating placeholder targets (PREDICTION MODE)...\")\n",
    "print(f\"  Build ID: {BUILD_ID}\")\n",
    "print(f\"  Max length: {MAX_LEN}\")\n",
    "print(f\"  Num variables: {NUM_VARS}\")\n",
    "print()\n",
    "\n",
    "df_trg = generate_ist_placeholders(\n",
    "    group_ids=PREDICTION_GROUPS,\n",
    "    max_len=MAX_LEN,\n",
    "    num_vars=NUM_VARS,\n",
    "    output_path=TARGET_BUILD_DIR  # Saves df_trg.csv here\n",
    ")\n",
    "\n",
    "print(\"\\nPlaceholder target generation complete!\")\n",
    "print(f\"Output: data/target/builds/{BUILD_ID}/df_trg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\target\\builds\\prediction_tutorial\\df_trg.csv\n",
      "\n",
      "Target dataframe shape: (6400, 5)\n",
      "Unique groups (samples): 16\n",
      "Unique variables: [1 2]\n",
      "Value range: [0.0, 0.0] (should be all zeros)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "position",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "variable",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "value",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2685f0a5-88b4-43d7-bbad-93a5eb97d2b5",
       "rows": [
        [
         "0",
         "CYDH_01",
         "1",
         "2026-02-03 16:45:21",
         "1",
         "0.0"
        ],
        [
         "1",
         "CYDH_01",
         "2",
         "2026-02-03 16:45:21",
         "1",
         "0.0"
        ],
        [
         "2",
         "CYDH_01",
         "3",
         "2026-02-03 16:45:21",
         "1",
         "0.0"
        ],
        [
         "3",
         "CYDH_01",
         "4",
         "2026-02-03 16:45:21",
         "1",
         "0.0"
        ],
        [
         "4",
         "CYDH_01",
         "5",
         "2026-02-03 16:45:21",
         "1",
         "0.0"
        ],
        [
         "5",
         "CYDH_01",
         "6",
         "2026-02-03 16:45:21",
         "1",
         "0.0"
        ],
        [
         "6",
         "CYDH_01",
         "7",
         "2026-02-03 16:45:21",
         "1",
         "0.0"
        ],
        [
         "7",
         "CYDH_01",
         "8",
         "2026-02-03 16:45:21",
         "1",
         "0.0"
        ],
        [
         "8",
         "CYDH_01",
         "9",
         "2026-02-03 16:45:21",
         "1",
         "0.0"
        ],
        [
         "9",
         "CYDH_01",
         "10",
         "2026-02-03 16:45:21",
         "1",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>position</th>\n",
       "      <th>date</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CYDH_01</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-02-03 16:45:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CYDH_01</td>\n",
       "      <td>2</td>\n",
       "      <td>2026-02-03 16:45:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CYDH_01</td>\n",
       "      <td>3</td>\n",
       "      <td>2026-02-03 16:45:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CYDH_01</td>\n",
       "      <td>4</td>\n",
       "      <td>2026-02-03 16:45:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CYDH_01</td>\n",
       "      <td>5</td>\n",
       "      <td>2026-02-03 16:45:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CYDH_01</td>\n",
       "      <td>6</td>\n",
       "      <td>2026-02-03 16:45:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CYDH_01</td>\n",
       "      <td>7</td>\n",
       "      <td>2026-02-03 16:45:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CYDH_01</td>\n",
       "      <td>8</td>\n",
       "      <td>2026-02-03 16:45:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CYDH_01</td>\n",
       "      <td>9</td>\n",
       "      <td>2026-02-03 16:45:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CYDH_01</td>\n",
       "      <td>10</td>\n",
       "      <td>2026-02-03 16:45:21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group  position                 date  variable  value\n",
       "0  CYDH_01         1  2026-02-03 16:45:21         1    0.0\n",
       "1  CYDH_01         2  2026-02-03 16:45:21         1    0.0\n",
       "2  CYDH_01         3  2026-02-03 16:45:21         1    0.0\n",
       "3  CYDH_01         4  2026-02-03 16:45:21         1    0.0\n",
       "4  CYDH_01         5  2026-02-03 16:45:21         1    0.0\n",
       "5  CYDH_01         6  2026-02-03 16:45:21         1    0.0\n",
       "6  CYDH_01         7  2026-02-03 16:45:21         1    0.0\n",
       "7  CYDH_01         8  2026-02-03 16:45:21         1    0.0\n",
       "8  CYDH_01         9  2026-02-03 16:45:21         1    0.0\n",
       "9  CYDH_01        10  2026-02-03 16:45:21         1    0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify output and preview\n",
    "import pandas as pd\n",
    "\n",
    "df_trg_path = join(TARGET_BUILDS, BUILD_ID, \"df_trg.csv\")\n",
    "print(f\"Loading: {df_trg_path}\")\n",
    "\n",
    "df_trg = pd.read_csv(df_trg_path)\n",
    "print(f\"\\nTarget dataframe shape: {df_trg.shape}\")\n",
    "print(f\"Unique groups (samples): {df_trg['group'].nunique()}\")\n",
    "print(f\"Unique variables: {df_trg['variable'].unique()}\")\n",
    "print(f\"Value range: [{df_trg['value'].min()}, {df_trg['value'].max()}] (should be all zeros)\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_trg.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Copy Target to Build Folder\n",
    "\n",
    "The input pipeline expects `df_trg.csv` in the build's control folder. We need to copy it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build directory: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\builds\\prediction_tutorial\n",
      "Control directory: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\builds\\prediction_tutorial\\control\n",
      "Output directory: c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\builds\\prediction_tutorial\\output\n"
     ]
    }
   ],
   "source": [
    "# Configuration for input pipeline\n",
    "DATASET_ID = \"prediction_tutorial\"    # Must match or create this folder in data/builds/\n",
    "\n",
    "# Create build folder structure if it doesn't exist\n",
    "BUILD_DIR = join(ROOT, \"data\", \"builds\", DATASET_ID)\n",
    "CONTROL_DIR = join(BUILD_DIR, \"control\")\n",
    "OUTPUT_DIR = join(BUILD_DIR, \"output\")\n",
    "\n",
    "os.makedirs(CONTROL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Build directory: {BUILD_DIR}\")\n",
    "print(f\"Control directory: {CONTROL_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied df_trg.csv to c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\data\\builds\\prediction_tutorial\\control\\df_trg.csv\n"
     ]
    }
   ],
   "source": [
    "# Copy df_trg.csv to control folder\n",
    "import shutil\n",
    "\n",
    "source_path = join(TARGET_BUILDS, BUILD_ID, \"df_trg.csv\")\n",
    "dest_path = join(CONTROL_DIR, \"df_trg.csv\")\n",
    "\n",
    "if exists(source_path):\n",
    "    shutil.copy(source_path, dest_path)\n",
    "    print(f\"Copied df_trg.csv to {dest_path}\")\n",
    "else:\n",
    "    print(f\"ERROR: Source file not found: {source_path}\")\n",
    "    print(\"Make sure the placeholder generation ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Control Files\n",
    "\n",
    "The input pipeline requires several control files. You should copy these from an existing build or create them according to the schema in `data/builds/template_build/control/README.md`.\n",
    "\n",
    "**Required files:**\n",
    "- `config.yaml` - Points to input dataset folder\n",
    "- `lookup_selected.xlsx` - Variable selection per process\n",
    "- `steps_selected.xlsx` - Process steps to include\n",
    "- `Prozessfolgen_MSEI.xlsx` - Layer/occurrence mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking control files:\n",
      "  ✓ config.yaml\n",
      "  ✓ df_trg.csv\n",
      "  ✓ lookup_selected.xlsx\n",
      "  ✓ steps_selected.xlsx\n",
      "  ✓ Prozessfolgen_MSEI.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Check for required control files\n",
    "required_files = [\n",
    "    \"config.yaml\",\n",
    "    \"df_trg.csv\",\n",
    "    \"lookup_selected.xlsx\",\n",
    "    \"steps_selected.xlsx\",\n",
    "    \"Prozessfolgen_MSEI.xlsx\"\n",
    "]\n",
    "\n",
    "print(\"Checking control files:\")\n",
    "all_present = True\n",
    "for f in required_files:\n",
    "    path = join(CONTROL_DIR, f)\n",
    "    status = \"✓\" if exists(path) else \"✗ MISSING\"\n",
    "    if not exists(path):\n",
    "        all_present = False\n",
    "    print(f\"  {status} {f}\")\n",
    "\n",
    "if not all_present:\n",
    "    print(\"\\n⚠️ Some control files are missing!\")\n",
    "    print(\"Copy them from an existing build or create them following the schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Input Pipeline\n",
    "\n",
    "Now we can run the input pipeline to process manufacturing data and generate the final dataset.\n",
    "\n",
    "⚠️ **PREDICTION MODE DIFFERENCE**: We disable stratified splitting since this is for prediction, not training.\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "| Parameter | Description | Prediction Value |\n",
    "|-----------|-------------|------------------|\n",
    "| `dataset_id` | Build folder name | Same as your folder in data/builds/ |\n",
    "| `missing_threshold` | Max % missing values per variable | 30 (same as training) |\n",
    "| `use_stratified_split` | Enable stratified train/test split | **False** (disabled) |\n",
    "| `grouping_method` | How samples are grouped | \"panel\" (must match training) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pipeline configuration (PREDICTION MODE)\n",
    "MISSING_THRESHOLD = 30                # Remove variables with >30% missing values\n",
    "USE_STRATIFIED_SPLIT = False          # ⚠️ DISABLED for prediction mode\n",
    "DEBUG = False                         # Set True for quick test with subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running input (process) pipeline...\n",
      "  Dataset ID: prediction_tutorial\n",
      "  Missing threshold: 30%\n",
      "  Stratified split: False (disabled for prediction)\n",
      "\n",
      "Galvanic WARNING: UCL_Spüle Bakterienbefall-1.04 not in the columns\n",
      "Error occurred 'UCL_Spüle Bakterienbefall-1.04'\n",
      "Galvanic WARNING: UCL_Galv. Cu Cl--1.11/1.12 not in the columns\n",
      "Error occurred 'UCL_Galv. Cu Cl--1.11/1.12'\n",
      "Galvanic WARNING: UCL_Galv. Cu Cl--1.09/1.10 not in the columns\n",
      "Error occurred 'UCL_Galv. Cu Cl--1.09/1.10'\n",
      "Galvanic WARNING: 25TrocknerUmsetzzeit not in the columns\n",
      "Error occurred '25TrocknerUmsetzzeit'\n",
      "Galvanic WARNING: 25TrocknerDelta_time not in the columns\n",
      "Error occurred '25TrocknerDelta_time'\n",
      "Galvanic WARNING: 25TrocknerDelta_time_% not in the columns\n",
      "Error occurred '25TrocknerDelta_time_%'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\proT_pipeline\\input_processing\\data_loader.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_['numeric_part'] = df_[process.panel_label].astype(str).str.extract(r'(\\d+)')[0].astype(\"Int64\")\n",
      "c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\proT_pipeline\\input_processing\\data_loader.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_[trans_group_id] = df_[process.WA_label] + '_' + df_['numeric_part'].astype(str)\n",
      "c:\\Users\\ScipioneFrancesco\\Documents\\Projects\\proT_pipeline\\proT_pipeline\\input_processing\\data_loader.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_[trans_group_id] = df_[process.WA_label] + '_*'\n",
      "100%|██████████| 16/16 [00:00<00:00, 548.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening successful, dataset correctly generated!\n",
      "Found the following sequence lengths\n",
      "        length_count                        ids\n",
      "length                                         \n",
      "320                3                    0, 1, 8\n",
      "925                3                    5, 6, 7\n",
      "933                3                    2, 3, 4\n",
      "1155               7  9, 10, 11, 12, 13, 14, 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 815.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening successful, dataset correctly generated!\n",
      "Found the following sequence lengths\n",
      "        length_count                                                ids\n",
      "length                                                                 \n",
      "400               16  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ...\n",
      "\n",
      "Input pipeline complete!\n",
      "Output: data/builds/prediction_tutorial/output/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run input pipeline\n",
    "from proT_pipeline.main import main as input_main\n",
    "\n",
    "print(\"Running input (process) pipeline...\")\n",
    "print(f\"  Dataset ID: {DATASET_ID}\")\n",
    "print(f\"  Missing threshold: {MISSING_THRESHOLD}%\")\n",
    "print(f\"  Stratified split: {USE_STRATIFIED_SPLIT} (disabled for prediction)\")\n",
    "print()\n",
    "\n",
    "input_main(\n",
    "    dataset_id=DATASET_ID,\n",
    "    missing_threshold=MISSING_THRESHOLD,\n",
    "    select_test=False,\n",
    "    use_stratified_split=USE_STRATIFIED_SPLIT,\n",
    "    grouping_method=GROUPING_METHOD,\n",
    "    grouping_column=None,\n",
    "    debug=DEBUG\n",
    ")\n",
    "\n",
    "print(\"\\nInput pipeline complete!\")\n",
    "print(f\"Output: data/builds/{DATASET_ID}/output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Output\n",
    "\n",
    "Let's examine the generated dataset to ensure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "\n",
      "X (input) shape: (16, 1155, 12)\n",
      "  - Samples: 16\n",
      "  - Max sequence length: 1155\n",
      "  - Features: 12\n",
      "\n",
      "Y (target) shape: (16, 400, 9)\n",
      "  - Samples: 16\n",
      "  - Max sequence length: 400\n",
      "  - Features: 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the generated dataset\n",
    "dataset_path = join(OUTPUT_DIR, f\"ds_{DATASET_ID}\", \"data.npz\")\n",
    "\n",
    "if exists(dataset_path):\n",
    "    data = np.load(dataset_path)\n",
    "    X = data['x']\n",
    "    Y = data['y']\n",
    "    \n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"\\nX (input) shape: {X.shape}\")\n",
    "    print(f\"  - Samples: {X.shape[0]}\")\n",
    "    print(f\"  - Max sequence length: {X.shape[1]}\")\n",
    "    print(f\"  - Features: {X.shape[2]}\")\n",
    "    print(f\"\\nY (target) shape: {Y.shape}\")\n",
    "    print(f\"  - Samples: {Y.shape[0]}\")\n",
    "    print(f\"  - Max sequence length: {Y.shape[1]}\")\n",
    "    print(f\"  - Features: {Y.shape[2]}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary files:\n",
      "\n",
      "group_vocabulary.json: 16 entries\n",
      "  First 5: {'CYDH_01': 0, 'CYDH_05': 1, 'CYDH_13': 2, 'CYDH_15': 3, 'CYDH_22': 4}\n",
      "\n",
      "process_vocabulary.json: 5 entries\n",
      "  {'Multibond': 1, 'Microetch': 2, 'Laser': 3, 'Plasma': 4, 'Galvanic': 5}\n",
      "\n",
      "variables_vocabulary.json_input: 416 entries\n",
      "  First 5: {'mul_10': 1, 'mul_100': 2, 'mul_101': 3, 'mul_102': 4, 'mul_103': 5}\n",
      "\n",
      "variables_vocabulary.json_trg: 2 entries\n",
      "  {'1': 1, '2': 2}\n",
      "\n",
      "features_dict: 2 entries\n",
      "  {'input': {'0': 'group', '1': 'process', '2': 'occurrence', '3': 'step', '4': 'variable', '5': 'value_norm', '6': 'order', '7': 'year', '8': 'month', '9': 'day', '10': 'hour', '11': 'minute'}, 'target': {'0': 'group', '1': 'position', '2': 'variable', '3': 'value', '4': 'year', '5': 'month', '6': 'day', '7': 'hour', '8': 'minute'}}\n"
     ]
    }
   ],
   "source": [
    "# Check vocabulary files\n",
    "import json\n",
    "\n",
    "vocab_files = [\n",
    "    \"group_vocabulary.json\",\n",
    "    \"process_vocabulary.json\",\n",
    "    \"variables_vocabulary.json_input\",\n",
    "    \"variables_vocabulary.json_trg\",\n",
    "    \"features_dict\"\n",
    "]\n",
    "\n",
    "print(\"Vocabulary files:\")\n",
    "for vf in vocab_files:\n",
    "    path = join(OUTPUT_DIR, vf)\n",
    "    if exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        print(f\"\\n{vf}: {len(vocab)} entries\")\n",
    "        if len(vocab) <= 10:\n",
    "            print(f\"  {vocab}\")\n",
    "        else:\n",
    "            print(f\"  First 5: {dict(list(vocab.items())[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target values verification:\n",
      "  Zero count: 6400 / 6400 (100.0%)\n",
      "  ✓ Target contains placeholder zeros as expected for prediction mode\n"
     ]
    }
   ],
   "source": [
    "# Verify that Y contains placeholder values (zeros)\n",
    "# Y structure: [sample, position, features] where feature 3 is the value\n",
    "if exists(dataset_path):\n",
    "    y_values = Y[:, :, 3]  # Value column (index 3 based on features_dict)\n",
    "    \n",
    "    zero_count = (y_values == 0).sum()\n",
    "    total_count = y_values.size\n",
    "    zero_percentage = (zero_count / total_count) * 100\n",
    "    \n",
    "    print(f\"Target values verification:\")\n",
    "    print(f\"  Zero count: {zero_count} / {total_count} ({zero_percentage:.1f}%)\")\n",
    "    \n",
    "    if zero_percentage > 90:\n",
    "        print(\"  ✓ Target contains placeholder zeros as expected for prediction mode\")\n",
    "    else:\n",
    "        print(\"  ⚠️ Some non-zero values found (may be from padding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully generated a prediction dataset! The output includes:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `data.npz` | Full dataset (X with real process data, Y with placeholder zeros) |\n",
    "| `*_vocabulary.json` | Mapping dictionaries |\n",
    "| `features_dict` | Feature index documentation |\n",
    "| `sample_metrics.parquet` | Sample-level metrics for analysis |\n",
    "\n",
    "**Note**: No `train_data.npz` or `test_data.npz` since stratified split is disabled.\n",
    "\n",
    "## Using the Prediction Dataset\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "data = np.load('data/builds/prediction_tutorial/output/ds_prediction_tutorial/data.npz')\n",
    "X_pred = torch.from_numpy(data['x']).float()\n",
    "Y_placeholder = torch.from_numpy(data['y']).float()\n",
    "\n",
    "# Run inference with your trained model\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_pred, Y_placeholder)\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Load the prediction dataset into your trained transformer model\n",
    "2. Run inference to generate IST predictions\n",
    "3. Map predictions back to group names using `group_vocabulary.json`\n",
    "4. Refer to `INTEGRATION_GUIDE.md` for model integration details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prochain_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
