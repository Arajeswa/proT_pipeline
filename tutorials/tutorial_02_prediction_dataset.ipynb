{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Producing a Dataset for Prediction\n",
    "\n",
    "This tutorial guides you through generating a dataset for **inference/prediction** mode, where target (IST) data is not available.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dataset generation follows the same structure as Tutorial 1:\n",
    "1. **Target Pipeline (IST)**: ⚠️ **DIFFERENCE**: Instead of processing real IST data, we generate placeholders\n",
    "2. **Input Pipeline (Process)**: Processes manufacturing parameters (same as Tutorial 1)\n",
    "\n",
    "## Key Difference from Training\n",
    "\n",
    "| Aspect | Tutorial 1 (Training) | Tutorial 2 (Prediction) |\n",
    "|--------|----------------------|-------------------------|\n",
    "| Step 1 | Process real IST data | Generate placeholder IST |\n",
    "| Y values | Actual resistance deltas | Zeros (placeholders) |\n",
    "| Stratified split | Enabled | Disabled |\n",
    "| Purpose | Train model | Generate predictions |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- Installed all dependencies (`pip install -r requirements.txt`)\n",
    "- Obtained access to the raw process data files\n",
    "- Placed data files according to `data/README_DATA.md`\n",
    "- A list of sample groups (batches/panels) to generate predictions for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "First, let's set up the environment and verify the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import exists, join, dirname, abspath\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = dirname(abspath(os.getcwd()))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data structure\n",
    "from proT_pipeline.labels import get_target_dirs, get_input_dirs, get_root_dir\n",
    "\n",
    "ROOT = get_root_dir()\n",
    "TARGET_INPUT, TARGET_BUILDS = get_target_dirs(ROOT)\n",
    "\n",
    "print(\"Checking data directories...\")\n",
    "print(f\"  Target input dir: {TARGET_INPUT}\")\n",
    "print(f\"    Exists: {exists(TARGET_INPUT)}\")\n",
    "print(f\"  Target builds dir: {TARGET_BUILDS}\")\n",
    "print(f\"    Exists: {exists(TARGET_BUILDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Process Target Data (IST)\n",
    "\n",
    "⚠️ **PREDICTION MODE DIFFERENCE**: Instead of processing real IST data with `target_main()`, we generate **placeholder targets** using `generate_ist_placeholders()`.\n",
    "\n",
    "The placeholder targets have:\n",
    "- Same structure as real IST data (group, position, variable, value columns)\n",
    "- Zero values instead of actual resistance measurements\n",
    "- Same sequence length (`MAX_LEN`) as training data\n",
    "\n",
    "### Input Options for Group IDs\n",
    "\n",
    "The `generate_ist_placeholders` function accepts group IDs in multiple formats:\n",
    "- **List**: `[\"CYDH_01\", \"CYDH_05\", ...]`\n",
    "- **CSV file**: Path to file with 'group' column\n",
    "- **NPY file**: Path to numpy array\n",
    "- **TXT file**: Path to text file (one ID per line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for target pipeline (PREDICTION MODE)\n",
    "BUILD_ID = \"prediction_tutorial\"      # Change this to your build name\n",
    "GROUPING_METHOD = \"panel\"             # Group by panel (must match training)\n",
    "MAX_LEN = 200                         # Maximum sequence length (must match training)\n",
    "NUM_VARS = 2                          # Number of target variables (Sense A, B)\n",
    "\n",
    "# Reference build for copying control files (Step 3)\n",
    "REFERENCE_BUILD = \"dyconex_251117\"    # An existing build with control files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the group IDs to generate predictions for\n",
    "# Option A: Define as a list\n",
    "PREDICTION_GROUPS = [\n",
    "    \"CYDH_01\",\n",
    "    \"CYDH_05\",\n",
    "    \"CYDH_13\",\n",
    "    \"CYDH_15\",\n",
    "    \"CYDH_22\",\n",
    "    \"CYDH_28\",\n",
    "    \"CYDH_30\",\n",
    "    \"CYDH_38\",\n",
    "    \"CYEI_04\",\n",
    "    \"CYEI_10\",\n",
    "    \"CYEI_20\",\n",
    "    \"CYEI_26\",\n",
    "    \"CYEI_28\",\n",
    "    \"CYEI_29\",\n",
    "    \"CYEI_39\",\n",
    "    \"CYEI_41\",\n",
    "]\n",
    "\n",
    "# Option B: Load from file (uncomment to use)\n",
    "# PREDICTION_GROUPS = \"path/to/sample_ids.csv\"    # CSV with 'group' column\n",
    "# PREDICTION_GROUPS = \"path/to/sample_ids.npy\"    # NumPy array\n",
    "# PREDICTION_GROUPS = \"path/to/sample_ids.txt\"    # Text file, one ID per line\n",
    "\n",
    "# Option C: Load from existing dataset (uncomment to use)\n",
    "# from proT_pipeline.target_processing.placeholders import load_group_ids_from_process_data\n",
    "# PREDICTION_GROUPS = load_group_ids_from_process_data(\"dyconex_251117\", limit=20)\n",
    "\n",
    "print(f\"Groups to predict: {len(PREDICTION_GROUPS) if isinstance(PREDICTION_GROUPS, list) else PREDICTION_GROUPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate placeholder targets (PREDICTION MODE)\n",
    "# This replaces the target_main() call from Tutorial 1\n",
    "from proT_pipeline.target_processing.placeholders import generate_ist_placeholders\n",
    "from os import makedirs\n",
    "\n",
    "# Create target build directory\n",
    "TARGET_BUILD_DIR = join(TARGET_BUILDS, BUILD_ID)\n",
    "makedirs(TARGET_BUILD_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Generating placeholder targets (PREDICTION MODE)...\")\n",
    "print(f\"  Build ID: {BUILD_ID}\")\n",
    "print(f\"  Max length: {MAX_LEN}\")\n",
    "print(f\"  Num variables: {NUM_VARS}\")\n",
    "print()\n",
    "\n",
    "df_trg = generate_ist_placeholders(\n",
    "    group_ids=PREDICTION_GROUPS,\n",
    "    max_len=MAX_LEN,\n",
    "    num_vars=NUM_VARS,\n",
    "    output_path=TARGET_BUILD_DIR  # Saves df_trg.csv here\n",
    ")\n",
    "\n",
    "print(\"\\nPlaceholder target generation complete!\")\n",
    "print(f\"Output: data/target/builds/{BUILD_ID}/df_trg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify output and preview\n",
    "import pandas as pd\n",
    "\n",
    "df_trg_path = join(TARGET_BUILDS, BUILD_ID, \"df_trg.csv\")\n",
    "print(f\"Loading: {df_trg_path}\")\n",
    "\n",
    "df_trg = pd.read_csv(df_trg_path)\n",
    "print(f\"\\nTarget dataframe shape: {df_trg.shape}\")\n",
    "print(f\"Unique groups (samples): {df_trg['group'].nunique()}\")\n",
    "print(f\"Unique variables: {df_trg['variable'].unique()}\")\n",
    "print(f\"Value range: [{df_trg['value'].min()}, {df_trg['value'].max()}] (should be all zeros)\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_trg.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Copy Target to Build Folder\n",
    "\n",
    "The input pipeline expects `df_trg.csv` in the build's control folder. We need to copy it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for input pipeline\n",
    "DATASET_ID = \"prediction_tutorial\"    # Must match or create this folder in data/builds/\n",
    "\n",
    "# Create build folder structure if it doesn't exist\n",
    "BUILD_DIR = join(ROOT, \"data\", \"builds\", DATASET_ID)\n",
    "CONTROL_DIR = join(BUILD_DIR, \"control\")\n",
    "OUTPUT_DIR = join(BUILD_DIR, \"output\")\n",
    "\n",
    "os.makedirs(CONTROL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Build directory: {BUILD_DIR}\")\n",
    "print(f\"Control directory: {CONTROL_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df_trg.csv to control folder\n",
    "import shutil\n",
    "\n",
    "source_path = join(TARGET_BUILDS, BUILD_ID, \"df_trg.csv\")\n",
    "dest_path = join(CONTROL_DIR, \"df_trg.csv\")\n",
    "\n",
    "if exists(source_path):\n",
    "    shutil.copy(source_path, dest_path)\n",
    "    print(f\"Copied df_trg.csv to {dest_path}\")\n",
    "else:\n",
    "    print(f\"ERROR: Source file not found: {source_path}\")\n",
    "    print(\"Make sure the placeholder generation ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Control Files\n",
    "\n",
    "The input pipeline requires several control files. You should copy these from an existing build or create them according to the schema in `data/builds/template_build/control/README.md`.\n",
    "\n",
    "**Required files:**\n",
    "- `config.yaml` - Points to input dataset folder\n",
    "- `lookup_selected.xlsx` - Variable selection per process\n",
    "- `steps_selected.xlsx` - Process steps to include\n",
    "- `Prozessfolgen_MSEI.xlsx` - Layer/occurrence mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for required control files\n",
    "required_files = [\n",
    "    \"config.yaml\",\n",
    "    \"df_trg.csv\",\n",
    "    \"lookup_selected.xlsx\",\n",
    "    \"steps_selected.xlsx\",\n",
    "    \"Prozessfolgen_MSEI.xlsx\"\n",
    "]\n",
    "\n",
    "print(\"Checking control files:\")\n",
    "all_present = True\n",
    "for f in required_files:\n",
    "    path = join(CONTROL_DIR, f)\n",
    "    status = \"✓\" if exists(path) else \"✗ MISSING\"\n",
    "    if not exists(path):\n",
    "        all_present = False\n",
    "    print(f\"  {status} {f}\")\n",
    "\n",
    "if not all_present:\n",
    "    print(\"\\n⚠️ Some control files are missing!\")\n",
    "    print(\"Copy them from an existing build or create them following the schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Input Pipeline\n",
    "\n",
    "Now we can run the input pipeline to process manufacturing data and generate the final dataset.\n",
    "\n",
    "⚠️ **PREDICTION MODE DIFFERENCE**: We disable stratified splitting since this is for prediction, not training.\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "| Parameter | Description | Prediction Value |\n",
    "|-----------|-------------|------------------|\n",
    "| `dataset_id` | Build folder name | Same as your folder in data/builds/ |\n",
    "| `missing_threshold` | Max % missing values per variable | 30 (same as training) |\n",
    "| `use_stratified_split` | Enable stratified train/test split | **False** (disabled) |\n",
    "| `grouping_method` | How samples are grouped | \"panel\" (must match training) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pipeline configuration (PREDICTION MODE)\n",
    "MISSING_THRESHOLD = 30                # Remove variables with >30% missing values\n",
    "USE_STRATIFIED_SPLIT = False          # ⚠️ DISABLED for prediction mode\n",
    "DEBUG = False                         # Set True for quick test with subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run input pipeline\n",
    "from proT_pipeline.main import main as input_main\n",
    "\n",
    "print(\"Running input (process) pipeline...\")\n",
    "print(f\"  Dataset ID: {DATASET_ID}\")\n",
    "print(f\"  Missing threshold: {MISSING_THRESHOLD}%\")\n",
    "print(f\"  Stratified split: {USE_STRATIFIED_SPLIT} (disabled for prediction)\")\n",
    "print()\n",
    "\n",
    "input_main(\n",
    "    dataset_id=DATASET_ID,\n",
    "    missing_threshold=MISSING_THRESHOLD,\n",
    "    select_test=False,\n",
    "    use_stratified_split=USE_STRATIFIED_SPLIT,\n",
    "    grouping_method=GROUPING_METHOD,\n",
    "    grouping_column=None,\n",
    "    debug=DEBUG\n",
    ")\n",
    "\n",
    "print(\"\\nInput pipeline complete!\")\n",
    "print(f\"Output: data/builds/{DATASET_ID}/output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Output\n",
    "\n",
    "Let's examine the generated dataset to ensure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the generated dataset\n",
    "dataset_path = join(OUTPUT_DIR, f\"ds_{DATASET_ID}\", \"data.npz\")\n",
    "\n",
    "if exists(dataset_path):\n",
    "    data = np.load(dataset_path)\n",
    "    X = data['x']\n",
    "    Y = data['y']\n",
    "    \n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"\\nX (input) shape: {X.shape}\")\n",
    "    print(f\"  - Samples: {X.shape[0]}\")\n",
    "    print(f\"  - Max sequence length: {X.shape[1]}\")\n",
    "    print(f\"  - Features: {X.shape[2]}\")\n",
    "    print(f\"\\nY (target) shape: {Y.shape}\")\n",
    "    print(f\"  - Samples: {Y.shape[0]}\")\n",
    "    print(f\"  - Max sequence length: {Y.shape[1]}\")\n",
    "    print(f\"  - Features: {Y.shape[2]}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vocabulary files\n",
    "import json\n",
    "\n",
    "vocab_files = [\n",
    "    \"group_vocabulary.json\",\n",
    "    \"process_vocabulary.json\",\n",
    "    \"variables_vocabulary.json_input\",\n",
    "    \"variables_vocabulary.json_trg\",\n",
    "    \"features_dict\"\n",
    "]\n",
    "\n",
    "print(\"Vocabulary files:\")\n",
    "for vf in vocab_files:\n",
    "    path = join(OUTPUT_DIR, vf)\n",
    "    if exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        print(f\"\\n{vf}: {len(vocab)} entries\")\n",
    "        if len(vocab) <= 10:\n",
    "            print(f\"  {vocab}\")\n",
    "        else:\n",
    "            print(f\"  First 5: {dict(list(vocab.items())[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that Y contains placeholder values (zeros)\n",
    "# Y structure: [sample, position, features] where feature 3 is the value\n",
    "if exists(dataset_path):\n",
    "    y_values = Y[:, :, 3]  # Value column (index 3 based on features_dict)\n",
    "    \n",
    "    zero_count = (y_values == 0).sum()\n",
    "    total_count = y_values.size\n",
    "    zero_percentage = (zero_count / total_count) * 100\n",
    "    \n",
    "    print(f\"Target values verification:\")\n",
    "    print(f\"  Zero count: {zero_count} / {total_count} ({zero_percentage:.1f}%)\")\n",
    "    \n",
    "    if zero_percentage > 90:\n",
    "        print(\"  ✓ Target contains placeholder zeros as expected for prediction mode\")\n",
    "    else:\n",
    "        print(\"  ⚠️ Some non-zero values found (may be from padding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully generated a prediction dataset! The output includes:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `data.npz` | Full dataset (X with real process data, Y with placeholder zeros) |\n",
    "| `*_vocabulary.json` | Mapping dictionaries |\n",
    "| `features_dict` | Feature index documentation |\n",
    "| `sample_metrics.parquet` | Sample-level metrics for analysis |\n",
    "\n",
    "**Note**: No `train_data.npz` or `test_data.npz` since stratified split is disabled.\n",
    "\n",
    "## Using the Prediction Dataset\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "data = np.load('data/builds/prediction_tutorial/output/ds_prediction_tutorial/data.npz')\n",
    "X_pred = torch.from_numpy(data['x']).float()\n",
    "Y_placeholder = torch.from_numpy(data['y']).float()\n",
    "\n",
    "# Run inference with your trained model\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_pred, Y_placeholder)\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Load the prediction dataset into your trained transformer model\n",
    "2. Run inference to generate IST predictions\n",
    "3. Map predictions back to group names using `group_vocabulary.json`\n",
    "4. Refer to `INTEGRATION_GUIDE.md` for model integration details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
